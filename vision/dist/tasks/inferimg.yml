description: Run an inference query with a vision model
ctx: 32768
prompt: |-
    {prompt}
model:
    name: minicpm-v:8b-2.6-q8_0
    template: chatml
models:
    granite2b:
        name: granite3.2-vision:2b-q8_0
        template: phi3
    qwen3vl:
        name: qwen2.5vl:3b-q8_0
        template: chatml
    llama11b:
        name: llama3.2-vision:latest
        template: llama3
    gemma12b:
        name: gemma3:12b
        template: gemma
    gemma27b:
        name: gemma3:27b
        template: gemma
    mistral24b:
        name: mistral-small3.1:24b
        template: mistral-system
inferParams:
    top_k: 0
    top_p: 1.0
    min_p: 0.05
    temperature: 0.2
    max_tokens: 16384
